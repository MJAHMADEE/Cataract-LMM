{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQnZFwE_0jFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grOKil6A0anQ",
        "outputId": "7418f12a-2cd1-4c19-d6d6-b56850a5a94a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class CustomCocoDataset(Dataset):\n",
        "    def __init__(self, root, annotation, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the images folder\n",
        "            annotation (str): Path to the COCO annotation JSON file\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.coco = COCO(annotation)\n",
        "        # Filter images to include only those with at least one annotation\n",
        "        self.ids = [\n",
        "            img_id\n",
        "            for img_id in self.coco.imgs.keys()\n",
        "            if len(self.coco.getAnnIds(imgIds=img_id)) > 0\n",
        "        ]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Load image\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Prepare targets\n",
        "        boxes, labels, masks, areas, iscrowd = [], [], [], [], []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "            masks.append(coco.annToMask(ann))\n",
        "            areas.append(ann[\"area\"])\n",
        "            iscrowd.append(ann[\"iscrowd\"])\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": torch.tensor([img_id]),\n",
        "            \"area\": areas,\n",
        "            \"iscrowd\": iscrowd,\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_transform():\n",
        "    return T.Compose(\n",
        "        [\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# Single dataset path (with only 'train' folder and annotation)\n",
        "data_root = \"/content/final_main_seg_dataset_just_ARAS-3/train/\"\n",
        "ann_file = os.path.join(data_root, \"_annotations.coco.json\")\n",
        "\n",
        "# Instantiate full dataset\n",
        "full_dataset = CustomCocoDataset(\n",
        "    root=data_root, annotation=ann_file, transform=get_transform()\n",
        ")\n",
        "\n",
        "total_samples = len(full_dataset)\n",
        "\n",
        "# Compute split sizes (no shuffling, preserve original order)\n",
        "train_size = int(0.7 * total_samples)\n",
        "valid_size = int(0.2 * total_samples)\n",
        "test_size = total_samples - train_size - valid_size\n",
        "\n",
        "# Generate index subsets\n",
        "train_indices = list(range(0, train_size))\n",
        "valid_indices = list(range(train_size, train_size + valid_size))\n",
        "test_indices = list(range(train_size + valid_size, total_samples))\n",
        "\n",
        "# Create subsets\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "valid_dataset = Subset(full_dataset, valid_indices)\n",
        "test_dataset = Subset(full_dataset, test_indices)\n",
        "\n",
        "\n",
        "# Custom collate function for batching\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "# DataLoaders with no shuffle\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyBOorrf0lB2",
        "outputId": "45d20a9f-f15d-49b8-ffe3-e9bcf2c779e3"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Load pre-trained Mask R-CNN with ResNet50-FPN backbone\n",
        "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Get input features for modifying the heads\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
        "\n",
        "# Replace the box predictor (13 classes: 12 categories + background)\n",
        "num_classes = 13  # Adjust this based on your dataset\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Replace the mask predictor\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "    in_features_mask, dim_reduced, num_classes\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeF2I46o0r-F"
      },
      "outputs": [],
      "source": [
        "# Parameters to optimize (only those requiring gradients)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Optimizer (SGD)\n",
        "# optimizer = torch.optim.SGD(\n",
        "#     params,\n",
        "#     lr=0.005,\n",
        "#     momentum=0.9,\n",
        "#     weight_decay=0.0005\n",
        "# )\n",
        "# Alternative: Use AdamW instead of SGD (uncomment to use)\n",
        "optimizer = torch.optim.AdamW(params, lr=0.0005, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=3, gamma=0.1  # Reduce LR every 3 epochs  # Multiply LR by 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "9wrEOfx40y_e",
        "outputId": "9c1de0c4-132d-4167-e094-40f310eea6f2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Assume model, train_loader, optimizer, lr_scheduler, and device are already defined\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # Wrap train_loader with tqdm for a progress bar\n",
        "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        # Move data to the appropriate device (e.g., GPU)\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)  # Returns a dictionary of losses\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    # Update learning rate (if using a scheduler)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-8WRosV01hV"
      },
      "outputs": [],
      "source": [
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/maskrcnn_finetuned.pth\")\n",
        "torch.save(model.state_dict(), \"maskrcnn_finetuned.pth\")\n",
        "print(\"Model saved as 'maskrcnn_finetuned.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPaVEHY607DF"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, test_ann_file, device):\n",
        "    model.eval()\n",
        "    coco_gt = COCO(test_ann_file)  # Ground truth annotations\n",
        "\n",
        "    # Prepare predictions in COCO format\n",
        "    coco_dt = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                image_id = targets[i][\"image_id\"].item()\n",
        "                boxes = output[\"boxes\"].cpu().numpy()\n",
        "                scores = output[\"scores\"].cpu().numpy()\n",
        "                labels = output[\"labels\"].cpu().numpy()\n",
        "                masks = output[\"masks\"].cpu().numpy()\n",
        "\n",
        "                for box, score, label, mask in zip(boxes, scores, labels, masks):\n",
        "                    # Convert box from [x_min, y_min, x_max, y_max] to [x, y, w, h]\n",
        "                    x_min, y_min, x_max, y_max = box\n",
        "                    x, y, w, h = x_min, y_min, x_max - x_min, y_max - y_min\n",
        "\n",
        "                    # Convert mask to RLE (Run-Length Encoding) for COCO\n",
        "                    mask = mask[\n",
        "                        0\n",
        "                    ]  # Mask R-CNN outputs [N, 1, H, W], take first channel\n",
        "                    mask = (mask > 0.5).astype(np.uint8)  # Binarize mask\n",
        "                    rle = coco_gt.maskUtils.encode(\n",
        "                        np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")\n",
        "                    )[0]\n",
        "                    rle[\"counts\"] = rle[\"counts\"].decode(\n",
        "                        \"utf-8\"\n",
        "                    )  # Convert bytes to string\n",
        "\n",
        "                    # Add prediction to list\n",
        "                    coco_dt.append(\n",
        "                        {\n",
        "                            \"image_id\": int(image_id),\n",
        "                            \"category_id\": int(label),\n",
        "                            \"bbox\": [float(x), float(y), float(w), float(h)],\n",
        "                            \"score\": float(score),\n",
        "                            \"segmentation\": rle,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    # Load predictions into COCO format\n",
        "    coco_dt = coco_gt.loadRes(coco_dt)\n",
        "\n",
        "    # Run COCO evaluation\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"segm\")  # Use 'segm' for mask evaluation\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    # Extract mAP@0.5 and mAP@0.5:0.95\n",
        "    mAP_50 = coco_eval.stats[1]  # AP at IoU=0.5\n",
        "    mAP_50_95 = coco_eval.stats[0]  # AP at IoU=0.5:0.95\n",
        "\n",
        "    print(f\"mAP@0.5: {mAP_50:.4f}\")\n",
        "    print(f\"mAP@0.5:0.95: {mAP_50_95:.4f}\")\n",
        "\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_model(model, test_loader, test_ann, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr39AWpS3aae"
      },
      "source": [
        "## Report mAP per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vnIhMgR3iPZ"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph1GYqGi2Dae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "\n",
        "# -------------------- COCO Dataset Definition --------------------\n",
        "class CustomCocoDataset(Dataset):\n",
        "    def __init__(self, root, annotation, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annotation)\n",
        "        self.ids = [\n",
        "            img_id\n",
        "            for img_id in self.coco.imgs.keys()\n",
        "            if len(self.coco.getAnnIds(imgIds=img_id)) > 0\n",
        "        ]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes, labels, masks, areas, iscrowd = [], [], [], [], []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "            masks.append(self.coco.annToMask(ann))\n",
        "            areas.append(ann[\"area\"])\n",
        "            iscrowd.append(ann[\"iscrowd\"])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": torch.tensor([img_id]),\n",
        "            \"area\": areas,\n",
        "            \"iscrowd\": iscrowd,\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "# -------------------- Transforms --------------------\n",
        "def get_transform():\n",
        "    return T.Compose(\n",
        "        [\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------- Data Splitting (70/20/10) --------------------\n",
        "data_root = \"train/\"\n",
        "ann_file = os.path.join(data_root, \"_annotations.coco.json\")\n",
        "full_dataset = CustomCocoDataset(\n",
        "    root=data_root, annotation=ann_file, transform=get_transform()\n",
        ")\n",
        "total = len(full_dataset)\n",
        "\n",
        "train_end = int(0.7 * total)\n",
        "valid_end = train_end + int(0.2 * total)\n",
        "\n",
        "train_dataset = Subset(full_dataset, list(range(0, train_end)))\n",
        "valid_dataset = Subset(full_dataset, list(range(train_end, valid_end)))\n",
        "test_dataset = Subset(full_dataset, list(range(valid_end, total)))\n",
        "\n",
        "# Collate fn\n",
        "collate_fn = lambda batch: tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# -------------------- Model Setup --------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = maskrcnn_resnet50_fpn(pretrained=False)\n",
        "num_classes = 13  # 12 classes + background\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(1024, num_classes)\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(256, 256, num_classes)\n",
        "model.load_state_dict(\n",
        "    torch.load(\"/content/maskrcnn_finetuned.pth\", map_location=device)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# -------------------- Evaluation --------------------\n",
        "def evaluate_model(model, data_loader, coco_annotation, data_root, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    # Metrics\n",
        "    metas = {\n",
        "        \"mask_full\": {\"iou_type\": \"segm\", \"iou_thresholds\": None},\n",
        "        \"mask_50\": {\"iou_type\": \"segm\", \"iou_thresholds\": [0.5]},\n",
        "        \"box_full\": {\"iou_type\": \"bbox\", \"iou_thresholds\": None},\n",
        "        \"box_50\": {\"iou_type\": \"bbox\", \"iou_thresholds\": [0.5]},\n",
        "    }\n",
        "    metrics = {}\n",
        "    for key, cfg in metas.items():\n",
        "        metrics[key] = MeanAveragePrecision(\n",
        "            box_format=\"xyxy\",\n",
        "            iou_type=cfg[\"iou_type\"],\n",
        "            iou_thresholds=cfg[\"iou_thresholds\"],\n",
        "            class_metrics=True,\n",
        "        ).to(\"cpu\")\n",
        "\n",
        "    coco_gt = COCO(coco_annotation)\n",
        "\n",
        "    # Visual params (using original fixed class colors and names)\n",
        "    class_colors = {\n",
        "        1: (178, 129, 241),\n",
        "        2: (111, 205, 151),\n",
        "        3: (188, 114, 0),\n",
        "        4: (54, 49, 173),\n",
        "        5: (104, 34, 237),\n",
        "        6: (30, 145, 246),\n",
        "        7: (153, 62, 98),\n",
        "        8: (85, 154, 0),\n",
        "        9: (220, 196, 71),\n",
        "        10: (115, 229, 223),\n",
        "        11: (0, 128, 128),\n",
        "        12: (0, 22, 103),\n",
        "    }\n",
        "    class_names = {\n",
        "        1: \"Cannula\",\n",
        "        2: \"Cap-Cystotome\",\n",
        "        3: \"Cap-Forceps\",\n",
        "        4: \"Cornea\",\n",
        "        5: \"Forceps\",\n",
        "        6: \"I-A-Handpiece\",\n",
        "        7: \"Lens-Injector\",\n",
        "        8: \"Phaco-Handpiece\",\n",
        "        9: \"Primary-Knife\",\n",
        "        10: \"Pupil\",\n",
        "        11: \"Second-Instrument\",\n",
        "        12: \"Secondary-Knife\",\n",
        "    }\n",
        "\n",
        "    with torch.no_grad()():\n",
        "        for images, targets in tqdm(data_loader, desc=\"Eval\"):\n",
        "            imgs = [img.to(device) for img in images]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for out, tgt in zip(outputs, targets):\n",
        "                img_id = tgt[\"image_id\"].item()\n",
        "                img_info = coco_gt.loadImgs(img_id)[0]\n",
        "                img_path = os.path.join(data_root, img_info[\"file_name\"])\n",
        "                img = cv2.imread(img_path)\n",
        "                vis = img.copy()\n",
        "\n",
        "                # Filter preds\n",
        "                keep = out[\"labels\"] != 0\n",
        "                preds = {\n",
        "                    \"boxes\": out[\"boxes\"][keep].cpu(),\n",
        "                    \"scores\": out[\"scores\"][keep].cpu(),\n",
        "                    \"labels\": (out[\"labels\"][keep] - 1).cpu(),\n",
        "                }\n",
        "                # For masks\n",
        "                pm = out[\"masks\"][keep].cpu().squeeze(1) > 0.5\n",
        "                preds_mask = {**preds, \"masks\": pm}\n",
        "\n",
        "                # GT\n",
        "                gt = {\n",
        "                    \"boxes\": tgt[\"boxes\"].cpu(),\n",
        "                    \"labels\": (tgt[\"labels\"] - 1).cpu(),\n",
        "                    \"masks\": tgt[\"masks\"].cpu().bool(),\n",
        "                }\n",
        "\n",
        "                # Update metrics\n",
        "                metrics[\"box_full\"].update([preds], [gt])\n",
        "                metrics[\"box_50\"].update([preds], [gt])\n",
        "                metrics[\"mask_full\"].update([preds_mask], [gt])\n",
        "                metrics[\"mask_50\"].update([preds_mask], [gt])\n",
        "\n",
        "                # Optional: save visualizations\n",
        "                for box, label, mask in zip(\n",
        "                    out[\"boxes\"][keep], out[\"labels\"][keep], pm\n",
        "                ):\n",
        "                    c = class_colors[int(label)]\n",
        "                    x1, y1, x2, y2 = map(int, box)\n",
        "                    vis[mask.numpy()] = cv2.addWeighted(\n",
        "                        vis[mask.numpy()], 0.5, np.full_like(vis, c), 0.5, 0\n",
        "                    )\n",
        "                    cv2.rectangle(vis, (x1, y1), (x2, y2), c, 2)\n",
        "                    cv2.putText(\n",
        "                        vis,\n",
        "                        cat_info[int(label)],\n",
        "                        (x1, y1 - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.5,\n",
        "                        c,\n",
        "                        1,\n",
        "                    )\n",
        "                cv2.imwrite(os.path.join(output_folder, img_info[\"file_name\"]), vis)\n",
        "\n",
        "    # Compute and print\n",
        "    results = {k: m.compute() for k, m in metrics.items()}\n",
        "    # Per-class and overall report\n",
        "    valid_ids = coco_gt.getCatIds()\n",
        "    names = cat_info\n",
        "\n",
        "    print(\"\\n====== Per-class mAP ======\")\n",
        "    for cid in valid_ids:\n",
        "        idx = cid - 1\n",
        "        name = names[cid]\n",
        "        print(\n",
        "            f\"{name}: Box@50 {results['box_50']['map_per_class'][idx]:.4f}, Box@full {results['box_full']['map_per_class'][idx]:.4f} | \"\n",
        "            f\"Mask@50 {results['mask_50']['map_per_class'][idx]:.4f}, Mask@full {results['mask_full']['map_per_class'][idx]:.4f}\"\n",
        "        )\n",
        "\n",
        "    print(\"\\n====== Overall mAP ======\")\n",
        "    for k in [\"box_50\", \"box_full\", \"mask_50\", \"mask_full\"]:\n",
        "        print(f\"{k}: {results[k]['map'].item():.4f}\")\n",
        "\n",
        "\n",
        "# -------------------- Execute Evaluation --------------------\n",
        "output_folder = \"output\"\n",
        "evaluate_model(model, test_loader, ann_file, data_root, output_folder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}