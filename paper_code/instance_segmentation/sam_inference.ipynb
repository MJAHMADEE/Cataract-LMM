{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77f6883e",
      "metadata": {},
      "source": [
        "# ðŸ”¬ SAM Inference for Surgical Instance Segmentation\n",
        "\n",
        "This notebook demonstrates the use of Segment Anything Model (SAM) for surgical instrument segmentation in cataract surgery videos.\n",
        "\n",
        "## Overview\n",
        "- Load pre-trained SAM model\n",
        "- Process surgical images\n",
        "- Generate segmentation masks\n",
        "- Evaluate performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317d264d",
      "metadata": {},
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26be6a52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python pycocotools matplotlib numpy torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "382f0d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download SAM checkpoint\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfd3127",
      "metadata": {},
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236d0c7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "# Import SAM components\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0248d2",
      "metadata": {},
      "source": [
        "## Initialize SAM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e91d9ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"  # Path to SAM checkpoint\n",
        "model_type = \"vit_h\"  # Model type\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load SAM model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Create predictor\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "print(f\"SAM model loaded on {device}\")\n",
        "print(f\"Model type: {model_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3da391e",
      "metadata": {},
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c753eb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_to_bbox(mask):\n",
        "    \"\"\"Convert binary mask to bounding box coordinates.\"\"\"\n",
        "    ys, xs = np.where(mask)\n",
        "    if len(xs) == 0 or len(ys) == 0:\n",
        "        return [0, 0, 0, 0]\n",
        "    x_min, x_max = int(xs.min()), int(xs.max())\n",
        "    y_min, y_max = int(ys.min()), int(ys.max())\n",
        "    return [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    \"\"\"Display segmentation mask with color overlay.\"\"\"\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    \"\"\"Display prompt points on the image.\"\"\"\n",
        "    pos_points = coords[labels == 1]\n",
        "    neg_points = coords[labels == 0]\n",
        "    ax.scatter(\n",
        "        pos_points[:, 0],\n",
        "        pos_points[:, 1],\n",
        "        color=\"green\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "    ax.scatter(\n",
        "        neg_points[:, 0],\n",
        "        neg_points[:, 1],\n",
        "        color=\"red\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    \"\"\"Display bounding box on the image.\"\"\"\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Utility functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd45161",
      "metadata": {},
      "source": [
        "## SAM Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af706de5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example inference function\n",
        "def run_sam_inference(image_path, input_points=None, input_boxes=None):\n",
        "    \"\"\"Run SAM inference on a single image.\"\"\"\n",
        "    # Load and process image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Set image for predictor\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    # Prepare inputs\n",
        "    input_labels = None\n",
        "    if input_points is not None:\n",
        "        input_labels = np.array([1] * len(input_points))  # All positive points\n",
        "        input_points = np.array(input_points)\n",
        "\n",
        "    # Generate masks\n",
        "    masks, scores, logits = predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        box=input_boxes,\n",
        "        multimask_output=True,\n",
        "    )\n",
        "\n",
        "    return image, masks, scores, logits\n",
        "\n",
        "\n",
        "# Visualization function\n",
        "def visualize_results(image, masks, scores, input_points=None, input_boxes=None):\n",
        "    \"\"\"Visualize SAM results.\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(masks), figsize=(15, 5))\n",
        "    if len(masks) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        axes[i].imshow(image)\n",
        "        show_mask(mask, axes[i])\n",
        "\n",
        "        if input_points is not None:\n",
        "            show_points(input_points, np.ones(len(input_points)), axes[i])\n",
        "\n",
        "        if input_boxes is not None:\n",
        "            show_box(input_boxes, axes[i])\n",
        "\n",
        "        axes[i].set_title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"SAM inference functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8a6590",
      "metadata": {},
      "source": [
        "## Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557feab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_iou(mask1, mask2):\n",
        "    \"\"\"Calculate Intersection over Union (IoU) between two masks.\"\"\"\n",
        "    intersection = np.logical_and(mask1, mask2).sum()\n",
        "    union = np.logical_or(mask1, mask2).sum()\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "\n",
        "def evaluate_segmentation(pred_masks, gt_masks):\n",
        "    \"\"\"Evaluate segmentation performance.\"\"\"\n",
        "    ious = []\n",
        "    for pred_mask, gt_mask in zip(pred_masks, gt_masks):\n",
        "        iou = calculate_iou(pred_mask, gt_mask)\n",
        "        ious.append(iou)\n",
        "\n",
        "    mean_iou = np.mean(ious)\n",
        "    return mean_iou, ious\n",
        "\n",
        "\n",
        "# Example evaluation\n",
        "print(\"Evaluation functions defined!\")\n",
        "print(\"Ready for SAM inference on surgical images!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020c0f48",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete workflow for using SAM (Segment Anything Model) for surgical instance segmentation:\n",
        "\n",
        "1. **Model Loading**: Load pre-trained SAM checkpoint\n",
        "2. **Inference**: Run segmentation with point/box prompts\n",
        "3. **Visualization**: Display results with overlays\n",
        "4. **Evaluation**: Calculate performance metrics\n",
        "\n",
        "The implementation follows the exact methodology described in the Cataract-LMM research paper for surgical instrument segmentation tasks."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}