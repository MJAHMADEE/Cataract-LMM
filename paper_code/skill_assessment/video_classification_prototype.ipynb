{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3fe3c681f6764ef691de67af63d83eb8",
            "d30c73ad5c6b4bbe917815d782642674",
            "36ce2194ee6a485fb22c90ccfa1bcb0c",
            "225cdfeb01c34802b31415e325e5025e",
            "37775daf741f4759b6f427970194177b",
            "b82e9152aeae4779b0d29f6da5e83637",
            "2a37141e39294c28b5bbb1d8d321b7e0",
            "6a5a0bf091a6483ba41197d632c81087",
            "eb0ec1e47552428884c3156520f0dc1c",
            "03e79bbf1ce44ebcbe0ec0a0276a2e59",
            "5bc15ef1c93f4e4a8fc45740455bb697",
            "e293d8cf819948a185d2d6a0c46b8ae7",
            "456d5855f3c64a7ab2c25da7a60e25ac",
            "287a6d0206ae452f83181dea01cd170b",
            "0753ed74c303494898121b09a3177a23",
            "391d17e5c6ac4be2a9017072b00f9edd",
            "442b62aa1c3a4f4c9a2f7cff0744b60a",
            "cb1fd6f0815647d0a4b85a23472fc8da",
            "158a42bada574229867c71a1673d3c8c",
            "595fcb398fe3465ba0fb254c6665ce33",
            "0d3ed222cb6b4328979f5d5bb14654b5",
            "edb21c567f3d42f5948943defbc924bb",
            "1e83c899cd7946f5994722ea3c187abe",
            "7ec3ab9be9d04a548f09e8cc51a0a4be",
            "a99511d0758942019021598fa245948f",
            "873a6bd06a164597ba75449c1917a753",
            "ef1ae9de59f6478f82bb90085dcab828",
            "05b5c997755f41ab968a770c978d0ecf",
            "c7b6b2b248d24af0b29cf3bb7ac77b0d",
            "f0bc2e7a94604bd1a17e7d75d907533e"
          ]
        },
        "id": "kiO1UrNkj2eJ",
        "outputId": "b67fb170-727b-4d1c-f33c-f11b19134eeb"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# 📹 ULTIMATE TWO-CLASS VIDEO CLASSIFICATION - SINGLE CELL COLAB SCRIPT\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# 1️⃣ ───── INSTALL & IMPORTS ─────\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "\n",
        "# Install all required packages\n",
        "required_packages = [\n",
        "    \"pytorchvideo\",\n",
        "    \"timesformer-pytorch\",\n",
        "    \"torchmetrics\",\n",
        "    \"rich\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"scikit-learn\",\n",
        "    \"decord\",\n",
        "    \"PyYAML\",\n",
        "    \"einops\",  # Required for ViViT and other transformers\n",
        "    \"av\",  # Alternative video reader\n",
        "]\n",
        "\n",
        "print(\"🔧 Installing required packages...\")\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        install_package(package)\n",
        "        print(f\"✅ {package} installed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to install {package}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    print(\"📁 Google Drive mounted successfully\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Not running in Colab - Drive mount skipped\")\n",
        "    files = None\n",
        "\n",
        "# Core imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import VideoReader\n",
        "import torch.cuda.amp as amp\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC, ConfusionMatrix\n",
        "from torchmetrics.classification import BinarySpecificity\n",
        "from torchmetrics.classification import Recall\n",
        "\n",
        "# To use it for binary sensitivity, you would instantiate it like this:\n",
        "binary_sensitivity = Recall(task=\"binary\")\n",
        "\n",
        "import yaml\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress, TaskID\n",
        "from rich.table import Table\n",
        "import warnings\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import time\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "from einops import rearrange, repeat  # For transformer models\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 2️⃣ ───── PARSE YAML CONFIG ─────\n",
        "config_text = \"\"\"\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# 📋 CONFIGURATION FILE - COMPLETE DOCUMENTATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# This configuration file controls all aspects of the video classification pipeline.\n",
        "# Each section is thoroughly documented with exact keywords and valid options.\n",
        "\n",
        "# ───── PATHS CONFIGURATION ─────\n",
        "# Configure input/output directories for your project\n",
        "paths:\n",
        "  # Root directory containing your video dataset\n",
        "  # Structure must be: data_root/class_name/video_files.*\n",
        "  # Supported video formats: .mp4, .avi, .mov, .mkv\n",
        "  data_root: \"/content/drive/MyDrive/SkillDataset_2Clusters\"\n",
        "\n",
        "  # Directory where all outputs will be saved (logs, checkpoints, plots, predictions)\n",
        "  # A timestamped subdirectory will be created for each run\n",
        "  output_root: \"/content/drive/MyDrive/2ClusterVideos/outputs\"\n",
        "\n",
        "# ───── HARDWARE CONFIGURATION ─────\n",
        "# Control GPU usage and memory optimization\n",
        "hardware:\n",
        "  # Number of GPUs to use\n",
        "  # Options: 0 (CPU only), 1+ (number of CUDA devices to use)\n",
        "  gpus: 1\n",
        "\n",
        "  # Enable Automatic Mixed Precision (AMP) for faster training and lower memory usage\n",
        "  # Options: true, false\n",
        "  mixed_precision: true\n",
        "\n",
        "  # Soft GPU memory limit in GB - script will auto-tune batch size if exceeded\n",
        "  # Recommended: 8, 12, 16, 24, 40, 80 (based on your GPU)\n",
        "  max_gpu_mem_gb: 12\n",
        "\n",
        "# ───── DATA LOADING & AUGMENTATION ─────\n",
        "# Configure how videos are loaded and processed\n",
        "data:\n",
        "  # Frames per second to sample from videos\n",
        "  # Lower values = fewer frames, faster processing\n",
        "  # Typical values: 1, 5, 10, 15, 30\n",
        "  frame_rate: 10\n",
        "\n",
        "  # Number of frames per video snippet/clip\n",
        "  # Must be compatible with model architecture\n",
        "  # Common values: 8, 16, 32, 64, 128, 256\n",
        "  clip_len: 50\n",
        "\n",
        "  # Overlap between consecutive snippets (in frames)\n",
        "  # 0 = no overlap, clip_len/2 = 50% overlap\n",
        "  # Increases data but also training time\n",
        "  snippet_overlap: 10\n",
        "\n",
        "  # Number of CPU workers for data loading\n",
        "  # Recommended: 2-8 (depends on CPU cores)\n",
        "  num_workers: 4\n",
        "\n",
        "  # ───── TRAIN/VAL/TEST SPLIT CONFIGURATION ─────\n",
        "  # Two modes available for splitting your dataset:\n",
        "\n",
        "  # Split mode - controls how data is divided\n",
        "  # Options:\n",
        "  # - \"stratified\": Maintains class proportions across splits (uses global_split_pct)\n",
        "  # - \"manual\": Allows custom class ratios per split (uses class_ratios & manual_split_sizes)\n",
        "  split_mode: \"stratified\"\n",
        "\n",
        "  # ── STRATIFIED MODE SETTINGS ──\n",
        "  # Used only when split_mode = \"stratified\"\n",
        "  # Percentages must sum to 100\n",
        "  global_split_pct:\n",
        "    train: 80    # 70% of each class → training\n",
        "    val:   10    # 15% of each class → validation\n",
        "    test:  10    # 15% of each class → testing\n",
        "\n",
        "  # ── MANUAL MODE SETTINGS ──\n",
        "  # Used only when split_mode = \"manual\"\n",
        "\n",
        "  # Class ratios - controls relative proportion of each class within each split\n",
        "  # Values represent percentages and should sum to 100 for each split\n",
        "  # Example: {cluster_0: 80, cluster_1: 20} = 80% class 0, 20% class 1\n",
        "  class_ratios:\n",
        "    train: {cluster_0: 50, cluster_1: 50}    # Balanced training set\n",
        "    val: {cluster_0: 50, cluster_1: 50}      # Balanced validation set\n",
        "    test: {cluster_0: 50, cluster_1: 50}     # Balanced test set\n",
        "\n",
        "  # Total number of samples per split (used only in manual mode)\n",
        "  # Adjust based on your dataset size\n",
        "  manual_split_sizes:\n",
        "    train: 140   # Total training samples\n",
        "    val: 10      # Total validation samples\n",
        "    test: 10     # Total test samples\n",
        "\n",
        "# ───── MODEL CONFIGURATION ─────\n",
        "# Select and configure the neural network architecture\n",
        "model:\n",
        "  # Model architecture to use\n",
        "  # Available options (exact keywords):\n",
        "  #\n",
        "  # CNN-based models:\n",
        "  # - \"x3d_m\": X3D-Medium (efficient 3D CNN)\n",
        "  # - \"slow_r50\": Slow pathway ResNet-50\n",
        "  # - \"slowfast_r50\": SlowFast ResNet-50 (dual pathway)\n",
        "  # - \"r2plus1d\": R(2+1)D-18 (decomposed 3D convolutions)\n",
        "  # - \"r3d_18\": ResNet 3D-18 (full 3D convolutions)\n",
        "  #\n",
        "  # Hybrid CNN-RNN models:\n",
        "  # - \"cnn_lstm\": CNN backbone + Bidirectional LSTM\n",
        "  # - \"cnn_gru\": CNN backbone + Bidirectional GRU\n",
        "  #\n",
        "  # Transformer-based models:\n",
        "  # - \"timesformer\": TimeSformer (divided space-time attention)\n",
        "  # - \"mvit\": Multiscale Vision Transformer\n",
        "  # - \"videomae\": Video Masked Autoencoder V2\n",
        "  # - \"vivit\": Video Vision Transformer\n",
        "  model_name: \"x3d_m\"\n",
        "\n",
        "  # Freeze pretrained backbone weights (train only classifier head)\n",
        "  # Options: true (faster, less memory), false (better accuracy)\n",
        "  freeze_backbone: false\n",
        "\n",
        "  # Dropout rate for classifier head (0.0-1.0)\n",
        "  # Higher values = more regularization\n",
        "  dropout: 0.25\n",
        "\n",
        "# ───── TRAINING HYPERPARAMETERS ─────\n",
        "# Configure the training process\n",
        "train:\n",
        "  # Maximum number of training epochs\n",
        "  epochs: 25\n",
        "\n",
        "  # Batch size (will be auto-tuned if GPU memory exceeded)\n",
        "  batch_size: 4\n",
        "\n",
        "  # Learning rate (typical range: 1e-5 to 1e-2)\n",
        "  lr: 1.0e-4\n",
        "\n",
        "  # Weight decay for AdamW optimizer (L2 regularization)\n",
        "  weight_decay: 1.0e-4\n",
        "\n",
        "  # Learning rate scheduler\n",
        "  # Options: \"cosine\" (smooth decay), \"step\" (sudden drops)\n",
        "  scheduler: \"cosine\"\n",
        "\n",
        "  # Step scheduler parameters (only used if scheduler = \"step\")\n",
        "  step_size: 10    # Epochs between LR drops\n",
        "  gamma: 0.1       # LR multiplication factor\n",
        "\n",
        "  # Gradient accumulation steps (simulates larger batch size)\n",
        "  # Use values > 1 if running out of memory\n",
        "  gradient_accumulation: 1\n",
        "\n",
        "  # Early stopping patience (epochs without improvement)\n",
        "  # Set to -1 to disable early stopping\n",
        "  early_stop_patience: 5\n",
        "\n",
        "  # Random seed for reproducibility\n",
        "  seed: 42\n",
        "\n",
        "# ───── METRICS CONFIGURATION ─────\n",
        "# Metrics to track during training/evaluation\n",
        "# Order matters for display in logs and plots\n",
        "# Available metrics: loss, accuracy, precision, recall, f1, sensitivity, specificity, auc\n",
        "metrics: [loss, accuracy, precision, recall, f1, sensitivity, specificity, auc]\n",
        "\n",
        "# ───── LOGGING CONFIGURATION ─────\n",
        "# Control output verbosity and style\n",
        "logging:\n",
        "  # Print frequency (batches between console updates)\n",
        "  print_freq: 5\n",
        "\n",
        "  # Enable emoji in console output\n",
        "  emojis: true\n",
        "\n",
        "  # Enable colored console output\n",
        "  colour: true\n",
        "\n",
        "  # Save all console output to log file\n",
        "  save_stdout: true\n",
        "\n",
        "  # Save detailed per-sample predictions (JSON format)\n",
        "  save_detailed_predictions: true\n",
        "\n",
        "# ───── RUN MODES ─────\n",
        "# Toggle different pipeline stages\n",
        "modes:\n",
        "  # Train the model\n",
        "  run_training: true\n",
        "\n",
        "  # Evaluate best model on test set\n",
        "  run_eval: true\n",
        "\n",
        "  # Run inference demo on single video\n",
        "  run_inference: true\n",
        "\n",
        "  # Path to video for inference demo\n",
        "  inference_video: \"/content/drive/MyDrive/2CusterVideos/cluster_0/SK_0002_S1_1006_Capsulorhexis.avi\"\n",
        "\n",
        "# ───── CLASS HANDLING ─────\n",
        "# Override automatic class detection\n",
        "override:\n",
        "  # Custom class names (null = auto-detect from folder names)\n",
        "  # Example: [\"healthy\", \"diseased\"]\n",
        "  class_names: null\n",
        "\n",
        "  # Pretty names for display (maps class index to display name)\n",
        "  # Example: {0: \"Healthy 🌿\", 1: \"Diseased 🍂\"}\n",
        "  class_name_map: {}\n",
        "\"\"\"\n",
        "\n",
        "config = yaml.safe_load(config_text)\n",
        "\n",
        "# 3️⃣ ───── UTILITIES ─────\n",
        "console = Console()\n",
        "\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def check_gpu_memory() -> Tuple[bool, float]:\n",
        "    \"\"\"Check available GPU memory.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return True, gpu_mem\n",
        "    return False, 0.0\n",
        "\n",
        "\n",
        "def print_section(title: str, emoji: str = \"🔹\") -> None:\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    if config[\"logging\"][\"emojis\"]:\n",
        "        console.print(f\"\\n{emoji} ───── {title.upper()} ─────\", style=\"bold cyan\")\n",
        "    else:\n",
        "        console.print(f\"\\n───── {title.upper()} ─────\", style=\"bold cyan\")\n",
        "\n",
        "\n",
        "def setup_output_dirs(output_root: str) -> Dict[str, Path]:\n",
        "    \"\"\"Create output directory structure.\"\"\"\n",
        "    output_path = Path(output_root)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = output_path / f\"run_{timestamp}\"\n",
        "\n",
        "    dirs = {\n",
        "        \"root\": run_dir,\n",
        "        \"checkpoints\": run_dir / \"checkpoints\",\n",
        "        \"logs\": run_dir / \"logs\",\n",
        "        \"plots\": run_dir / \"plots\",\n",
        "        \"predictions\": run_dir / \"predictions\",\n",
        "    }\n",
        "\n",
        "    for dir_path in dirs.values():\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return dirs\n",
        "\n",
        "\n",
        "# 4️⃣ ───── SPLIT ENGINE (FIXED) ─────\n",
        "def create_splits(data_root: str, config: Dict) -> Dict:\n",
        "    \"\"\"Create train/val/test splits with proper manual mode support.\"\"\"\n",
        "    data_path = Path(data_root)\n",
        "\n",
        "    # Auto-detect classes\n",
        "    class_folders = [\n",
        "        d for d in data_path.iterdir() if d.is_dir() and not d.name.startswith(\".\")\n",
        "    ]\n",
        "    class_names = config[\"override\"][\"class_names\"] or sorted(\n",
        "        [d.name for d in class_folders]\n",
        "    )\n",
        "\n",
        "    # Collect all videos per class\n",
        "    videos_per_class = {}\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_folder = data_path / class_name\n",
        "        if not class_folder.exists():\n",
        "            continue\n",
        "\n",
        "        video_files = []\n",
        "        for ext in [\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"]:\n",
        "            video_files.extend(list(class_folder.glob(ext)))\n",
        "\n",
        "        videos_per_class[class_name] = {\"files\": video_files, \"class_idx\": class_idx}\n",
        "\n",
        "        console.print(f\"📁 Found {len(video_files)} videos in class '{class_name}'\")\n",
        "\n",
        "    splits = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "    if config[\"data\"][\"split_mode\"] == \"stratified\":\n",
        "        # Stratified mode - preserve original class distribution\n",
        "        for class_name, class_data in videos_per_class.items():\n",
        "            video_files = class_data[\"files\"]\n",
        "            class_idx = class_data[\"class_idx\"]\n",
        "            n_videos = len(video_files)\n",
        "\n",
        "            if n_videos == 0:\n",
        "                continue\n",
        "\n",
        "            # Shuffle videos\n",
        "            indices = list(range(n_videos))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            # Calculate split sizes\n",
        "            pct = config[\"data\"][\"global_split_pct\"]\n",
        "            n_train = int(n_videos * pct[\"train\"] / 100)\n",
        "            n_val = int(n_videos * pct[\"val\"] / 100)\n",
        "\n",
        "            # Split indices\n",
        "            train_idx = indices[:n_train]\n",
        "            val_idx = indices[n_train : n_train + n_val]\n",
        "            test_idx = indices[n_train + n_val :]\n",
        "\n",
        "            # Add to splits\n",
        "            for idx in train_idx:\n",
        "                splits[\"train\"].append(\n",
        "                    {\n",
        "                        \"video_path\": str(video_files[idx]),\n",
        "                        \"class_idx\": class_idx,\n",
        "                        \"class_name\": class_name,\n",
        "                    }\n",
        "                )\n",
        "            for idx in val_idx:\n",
        "                splits[\"val\"].append(\n",
        "                    {\n",
        "                        \"video_path\": str(video_files[idx]),\n",
        "                        \"class_idx\": class_idx,\n",
        "                        \"class_name\": class_name,\n",
        "                    }\n",
        "                )\n",
        "            for idx in test_idx:\n",
        "                splits[\"test\"].append(\n",
        "                    {\n",
        "                        \"video_path\": str(video_files[idx]),\n",
        "                        \"class_idx\": class_idx,\n",
        "                        \"class_name\": class_name,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    else:  # manual mode - control class distribution within each split\n",
        "        # For each split, calculate how many samples from each class\n",
        "        for split_name in [\"train\", \"val\", \"test\"]:\n",
        "            total_samples = config[\"data\"][\"manual_split_sizes\"][split_name]\n",
        "            class_ratios = config[\"data\"][\"class_ratios\"][split_name]\n",
        "\n",
        "            # Normalize ratios to sum to 100\n",
        "            ratio_sum = sum(class_ratios.values())\n",
        "\n",
        "            samples_added = 0\n",
        "            for class_name in class_names:\n",
        "                if class_name not in videos_per_class:\n",
        "                    continue\n",
        "\n",
        "                # Calculate number of samples for this class in this split\n",
        "                ratio = class_ratios.get(class_name, 0)\n",
        "                n_samples = int(total_samples * ratio / ratio_sum)\n",
        "\n",
        "                # Get available videos for this class\n",
        "                class_data = videos_per_class[class_name]\n",
        "                video_files = class_data[\"files\"]\n",
        "                class_idx = class_data[\"class_idx\"]\n",
        "\n",
        "                # Sample videos (with replacement if necessary)\n",
        "                if len(video_files) > 0:\n",
        "                    if n_samples > len(video_files):\n",
        "                        # Need to sample with replacement\n",
        "                        sampled_indices = np.random.choice(\n",
        "                            len(video_files), n_samples, replace=True\n",
        "                        )\n",
        "                    else:\n",
        "                        # Sample without replacement\n",
        "                        sampled_indices = np.random.choice(\n",
        "                            len(video_files), n_samples, replace=False\n",
        "                        )\n",
        "\n",
        "                    for idx in sampled_indices:\n",
        "                        splits[split_name].append(\n",
        "                            {\n",
        "                                \"video_path\": str(video_files[idx]),\n",
        "                                \"class_idx\": class_idx,\n",
        "                                \"class_name\": class_name,\n",
        "                            }\n",
        "                        )\n",
        "                        samples_added += 1\n",
        "\n",
        "            # Shuffle the split to mix classes\n",
        "            np.random.shuffle(splits[split_name])\n",
        "\n",
        "            console.print(\n",
        "                f\"📊 {split_name.capitalize()} split: {len(splits[split_name])} samples\"\n",
        "            )\n",
        "\n",
        "            # Print class distribution\n",
        "            class_counts = defaultdict(int)\n",
        "            for item in splits[split_name]:\n",
        "                class_counts[item[\"class_name\"]] += 1\n",
        "\n",
        "            for class_name, count in class_counts.items():\n",
        "                percentage = (\n",
        "                    (count / len(splits[split_name])) * 100 if splits[split_name] else 0\n",
        "                )\n",
        "                console.print(f\"   - {class_name}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "    # Save splits info\n",
        "    splits_info = {\n",
        "        \"splits\": splits,\n",
        "        \"class_names\": class_names,\n",
        "        \"num_classes\": len(class_names),\n",
        "        \"split_mode\": config[\"data\"][\"split_mode\"],\n",
        "        \"videos_per_class\": {k: len(v[\"files\"]) for k, v in videos_per_class.items()},\n",
        "    }\n",
        "\n",
        "    return splits_info\n",
        "\n",
        "\n",
        "# 5️⃣ ───── DATASET & DATALOADERS (FIXED WITH OVERLAP) ─────\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        video_list: List[Dict],\n",
        "        clip_len: int,\n",
        "        frame_rate: int,\n",
        "        overlap: int = 0,\n",
        "        augment: bool = False,\n",
        "    ):\n",
        "        self.video_list = video_list\n",
        "        self.clip_len = clip_len\n",
        "        self.frame_rate = frame_rate\n",
        "        self.overlap = overlap\n",
        "        self.augment = augment\n",
        "\n",
        "        # Calculate stride between snippets\n",
        "        self.stride = max(1, clip_len - overlap)\n",
        "\n",
        "        # Build snippet index\n",
        "        self.snippets = []\n",
        "        for video_info in video_list:\n",
        "            video_path = video_info[\"video_path\"]\n",
        "\n",
        "            # Get video duration to calculate snippets\n",
        "            # For simplicity, we'll add multiple snippets per video based on overlap\n",
        "            # In a real implementation, you'd get the actual video length\n",
        "            if overlap > 0:\n",
        "                # Add multiple snippets per video\n",
        "                n_snippets = (\n",
        "                    3  # Simplified - in reality, calculate based on video length\n",
        "                )\n",
        "                for snippet_idx in range(n_snippets):\n",
        "                    self.snippets.append(\n",
        "                        {\n",
        "                            \"video_path\": video_path,\n",
        "                            \"class_idx\": video_info[\"class_idx\"],\n",
        "                            \"class_name\": video_info[\"class_name\"],\n",
        "                            \"snippet_idx\": snippet_idx,\n",
        "                            \"start_frame\": snippet_idx * self.stride,\n",
        "                        }\n",
        "                    )\n",
        "            else:\n",
        "                # No overlap - one snippet per video\n",
        "                self.snippets.append(\n",
        "                    {\n",
        "                        \"video_path\": video_path,\n",
        "                        \"class_idx\": video_info[\"class_idx\"],\n",
        "                        \"class_name\": video_info[\"class_name\"],\n",
        "                        \"snippet_idx\": 0,\n",
        "                        \"start_frame\": 0,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Normalization (ImageNet stats)\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]\n",
        "        )\n",
        "\n",
        "        # Augmentation transforms\n",
        "        if augment:\n",
        "            self.spatial_transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "                    transforms.ColorJitter(\n",
        "                        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.spatial_transform = transforms.Resize((224, 224))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.snippets)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, Dict]:\n",
        "        snippet_info = self.snippets[idx]\n",
        "        video_path = snippet_info[\"video_path\"]\n",
        "        label = snippet_info[\"class_idx\"]\n",
        "        start_frame = snippet_info[\"start_frame\"]\n",
        "\n",
        "        try:\n",
        "            # Load video using VideoReader\n",
        "            vr = VideoReader(video_path, \"video\")\n",
        "\n",
        "            video_frames = []\n",
        "\n",
        "            # Sample frames starting from start_frame\n",
        "            for i in range(self.clip_len):\n",
        "                frame_idx = start_frame + i\n",
        "                try:\n",
        "                    vr.seek(float(frame_idx))\n",
        "                    frame = next(vr)[\"data\"]  # (H, W, C)\n",
        "\n",
        "                    # Convert to PIL for transforms\n",
        "                    frame = transforms.ToPILImage()(frame)\n",
        "\n",
        "                    # Apply spatial transforms\n",
        "                    frame = self.spatial_transform(frame)\n",
        "\n",
        "                    # Convert back to tensor\n",
        "                    frame = transforms.ToTensor()(frame)\n",
        "\n",
        "                    video_frames.append(frame)\n",
        "                except:\n",
        "                    # If we can't read the frame, use the last valid frame\n",
        "                    if video_frames:\n",
        "                        video_frames.append(video_frames[-1].clone())\n",
        "                    else:\n",
        "                        # Create a black frame if no frames read yet\n",
        "                        video_frames.append(torch.zeros(3, 224, 224))\n",
        "\n",
        "            # Normalize frames\n",
        "            normalized_frames = [self.normalize(frame) for frame in video_frames]\n",
        "\n",
        "            # Stack frames\n",
        "            video_tensor = torch.stack(normalized_frames, dim=1)  # (C, T, H, W)\n",
        "\n",
        "            # Return tensor, label, and metadata\n",
        "            metadata = {\n",
        "                \"video_path\": video_path,\n",
        "                \"snippet_idx\": snippet_info[\"snippet_idx\"],\n",
        "                \"class_name\": snippet_info[\"class_name\"],\n",
        "            }\n",
        "\n",
        "            return video_tensor, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            console.print(f\"❌ Error processing video {video_path}: {e}\", style=\"red\")\n",
        "            # Return dummy tensor\n",
        "            dummy_frames = [torch.zeros(3, 224, 224) for _ in range(self.clip_len)]\n",
        "            normalized_frames = [self.normalize(f) for f in dummy_frames]\n",
        "            video_tensor = torch.stack(normalized_frames, dim=1)\n",
        "\n",
        "            metadata = {\n",
        "                \"video_path\": video_path,\n",
        "                \"snippet_idx\": snippet_info[\"snippet_idx\"],\n",
        "                \"class_name\": snippet_info[\"class_name\"],\n",
        "            }\n",
        "\n",
        "            return video_tensor, label, metadata\n",
        "\n",
        "\n",
        "# 6️⃣ ───── MODEL IMPLEMENTATIONS ─────\n",
        "\n",
        "\n",
        "# ===== CNN-RNN HYBRID MODELS =====\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    \"\"\"ResNet-based feature extractor for CNN-RNN models.\"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        # Use ResNet50 as backbone\n",
        "        resnet = torch.hub.load(\"pytorch/vision\", \"resnet50\", pretrained=pretrained)\n",
        "        # Remove the final FC layer\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.feature_dim = 2048\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C, T, H, W)\n",
        "        B, C, T, H, W = x.shape\n",
        "        # Process each frame through CNN\n",
        "        x = rearrange(x, \"b c t h w -> (b t) c h w\")\n",
        "        features = self.features(x)  # (B*T, 2048, 1, 1)\n",
        "        features = features.squeeze(-1).squeeze(-1)  # (B*T, 2048)\n",
        "        features = rearrange(features, \"(b t) d -> b t d\", b=B, t=T)\n",
        "        return features\n",
        "\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    \"\"\"CNN backbone with Bidirectional LSTM for temporal modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, hidden_dim=512, num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.cnn = CNNFeatureExtractor(pretrained=True)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.cnn.feature_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,  # Bidirectional as required\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 2 * hidden_dim due to bidirectional\n",
        "        self.classifier = nn.Linear(2 * hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract CNN features\n",
        "        features = self.cnn(x)  # (B, T, D)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (h_n, c_n) = self.lstm(features)  # lstm_out: (B, T, 2*hidden_dim)\n",
        "\n",
        "        # Use the last hidden state from both directions\n",
        "        # h_n shape: (num_layers * 2, B, hidden_dim)\n",
        "        h_forward = h_n[-2]  # Last layer, forward direction\n",
        "        h_backward = h_n[-1]  # Last layer, backward direction\n",
        "        h_combined = torch.cat([h_forward, h_backward], dim=1)  # (B, 2*hidden_dim)\n",
        "\n",
        "        # Classification\n",
        "        h_combined = self.dropout(h_combined)\n",
        "        output = self.classifier(h_combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CNN_GRU(nn.Module):\n",
        "    \"\"\"CNN backbone with Bidirectional GRU for temporal modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, hidden_dim=512, num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.cnn = CNNFeatureExtractor(pretrained=True)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.cnn.feature_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,  # Bidirectional as required\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 2 * hidden_dim due to bidirectional\n",
        "        self.classifier = nn.Linear(2 * hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract CNN features\n",
        "        features = self.cnn(x)  # (B, T, D)\n",
        "\n",
        "        # GRU processing\n",
        "        gru_out, h_n = self.gru(features)  # gru_out: (B, T, 2*hidden_dim)\n",
        "\n",
        "        # Use the last hidden state from both directions\n",
        "        # h_n shape: (num_layers * 2, B, hidden_dim)\n",
        "        h_forward = h_n[-2]  # Last layer, forward direction\n",
        "        h_backward = h_n[-1]  # Last layer, backward direction\n",
        "        h_combined = torch.cat([h_forward, h_backward], dim=1)  # (B, 2*hidden_dim)\n",
        "\n",
        "        # Classification\n",
        "        h_combined = self.dropout(h_combined)\n",
        "        output = self.classifier(h_combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ===== TRANSFORMER MODELS =====\n",
        "\n",
        "\n",
        "class MViT(nn.Module):\n",
        "    \"\"\"Multiscale Vision Transformer for video understanding.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        num_frames=16,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_frames = num_frames\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding with temporal dimension\n",
        "        self.patch_embed = nn.Conv3d(\n",
        "            3,\n",
        "            embed_dim,\n",
        "            kernel_size=(1, patch_size, patch_size),\n",
        "            stride=(1, patch_size, patch_size),\n",
        "        )\n",
        "\n",
        "        # Positional embeddings\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_frames * self.num_patches + 1, embed_dim)\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Transformer blocks with multi-scale pooling\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Pooling layers for multi-scale features\n",
        "        self.pool_layers = nn.ModuleList(\n",
        "            [\n",
        "                nn.MaxPool1d(kernel_size=2, stride=2) if i % 3 == 0 else nn.Identity()\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C, T, H, W)\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, T, H', W')\n",
        "        x = rearrange(x, \"b d t h w -> b (t h w) d\")\n",
        "\n",
        "        # Add cls token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_embed[:, : x.size(1)]\n",
        "\n",
        "        # Apply transformer blocks with multi-scale pooling\n",
        "        for i, (block, pool) in enumerate(zip(self.blocks, self.pool_layers)):\n",
        "            x = block(x)\n",
        "            if not isinstance(pool, nn.Identity) and x.size(1) > 1:\n",
        "                # Apply pooling to all tokens except cls token\n",
        "                cls_token, tokens = x[:, :1], x[:, 1:]\n",
        "                tokens = rearrange(tokens, \"b n d -> b d n\")\n",
        "                tokens = pool(tokens)\n",
        "                tokens = rearrange(tokens, \"b d n -> b n d\")\n",
        "                x = torch.cat([cls_token, tokens], dim=1)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Use cls token for classification\n",
        "        x = x[:, 0]\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Basic transformer block with self-attention and MLP.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VideoMAE(nn.Module):\n",
        "    \"\"\"Video Masked Autoencoder V2 - adapted for classification.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        num_frames=16,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        decoder_embed_dim=512,\n",
        "        decoder_depth=4,\n",
        "        decoder_num_heads=8,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        # Patch embedding (3D for video)\n",
        "        self.patch_embed = nn.Conv3d(\n",
        "            3,\n",
        "            embed_dim,\n",
        "            kernel_size=(2, patch_size, patch_size),\n",
        "            stride=(2, patch_size, patch_size),\n",
        "        )\n",
        "\n",
        "        num_patches = (num_frames // 2) * (img_size // patch_size) ** 2\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        # Positional embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C, T, H, W)\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, T', H', W')\n",
        "        x = rearrange(x, \"b d t h w -> b (t h w) d\")\n",
        "\n",
        "        # Add cls token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_embed[:, : x.size(1)]\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Use cls token for classification\n",
        "        x = x[:, 0]\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViViT(nn.Module):\n",
        "    \"\"\"Video Vision Transformer with factorized self-attention.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        num_frames=16,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_frames = num_frames\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Tubelet embedding (3D patches)\n",
        "        self.patch_embed = nn.Conv3d(\n",
        "            3,\n",
        "            embed_dim,\n",
        "            kernel_size=(2, patch_size, patch_size),\n",
        "            stride=(2, patch_size, patch_size),\n",
        "        )\n",
        "\n",
        "        # Calculate number of spatiotemporal patches\n",
        "        self.num_time_patches = num_frames // 2\n",
        "        self.num_space_patches = self.num_patches\n",
        "\n",
        "        # Positional embeddings (separate for space and time)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.space_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, self.num_space_patches, embed_dim)\n",
        "        )\n",
        "        self.time_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, self.num_time_patches, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks with factorized attention\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                FactorizedTransformerBlock(\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    self.num_time_patches,\n",
        "                    self.num_space_patches,\n",
        "                    mlp_ratio,\n",
        "                    dropout,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.space_pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.time_pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C, T, H, W)\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Tubelet embedding\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, T', H', W')\n",
        "        T_new, H_new, W_new = x.shape[2:]\n",
        "        x = rearrange(x, \"b d t h w -> b (t h w) d\")\n",
        "\n",
        "        # Add spatial and temporal position embeddings\n",
        "        space_pos = self.space_pos_embed.repeat(\n",
        "            1, T_new, 1\n",
        "        )  # Repeat for each time step\n",
        "        time_pos = self.time_pos_embed.repeat_interleave(\n",
        "            H_new * W_new, dim=1\n",
        "        )  # Repeat for each spatial position\n",
        "        x = x + space_pos[:, : x.size(1)] + time_pos[:, : x.size(1)]\n",
        "\n",
        "        # Add cls token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, T_new, H_new * W_new)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Use cls token for classification\n",
        "        x = x[:, 0]\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FactorizedTransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with factorized space-time attention.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        num_time_patches,\n",
        "        num_space_patches,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_time_patches = num_time_patches\n",
        "        self.num_space_patches = num_space_patches\n",
        "\n",
        "        # Spatial attention\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.spatial_attn = nn.MultiheadAttention(\n",
        "            dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Temporal attention\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.temporal_attn = nn.MultiheadAttention(\n",
        "            dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        # MLP\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, T, HW):\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # Separate cls token\n",
        "        cls_token, x_patches = x[:, :1], x[:, 1:]\n",
        "\n",
        "        # Spatial attention (within each frame)\n",
        "        x_spatial = rearrange(x_patches, \"b (t hw) d -> (b t) hw d\", t=T, hw=HW)\n",
        "        x_spatial = self.norm1(x_spatial)\n",
        "        x_spatial, _ = self.spatial_attn(x_spatial, x_spatial, x_spatial)\n",
        "        x_patches = x_patches + rearrange(\n",
        "            x_spatial, \"(b t) hw d -> b (t hw) d\", b=B, t=T\n",
        "        )\n",
        "\n",
        "        # Temporal attention (across frames)\n",
        "        x_temporal = rearrange(x_patches, \"b (t hw) d -> (b hw) t d\", t=T, hw=HW)\n",
        "        x_temporal = self.norm2(x_temporal)\n",
        "        x_temporal, _ = self.temporal_attn(x_temporal, x_temporal, x_temporal)\n",
        "        x_patches = x_patches + rearrange(\n",
        "            x_temporal, \"(b hw) t d -> b (t hw) d\", b=B, hw=HW\n",
        "        )\n",
        "\n",
        "        # Recombine with cls token\n",
        "        x = torch.cat([cls_token, x_patches], dim=1)\n",
        "\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.norm3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 7️⃣ ───── MODEL FACTORY (COMPLETED WITH ALL MODELS) ─────\n",
        "def create_model(\n",
        "    model_name: str,\n",
        "    num_classes: int,\n",
        "    clip_len: int,\n",
        "    freeze_backbone: bool = False,\n",
        "    dropout: float = 0.5,\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Create model based on model_name.\n",
        "\n",
        "    Available models:\n",
        "    - CNN-based: x3d_m, slow_r50, slowfast_r50, r2plus1d, r3d_18\n",
        "    - CNN-RNN: cnn_lstm, cnn_gru (both bidirectional)\n",
        "    - Transformer: timesformer, mvit, videomae, vivit\n",
        "    \"\"\"\n",
        "\n",
        "    # CNN-based models\n",
        "    if model_name == \"x3d_m\":\n",
        "        model = torch.hub.load(\n",
        "            \"facebookresearch/pytorchvideo\", \"x3d_m\", pretrained=True\n",
        "        )\n",
        "        in_features = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Sequential(\n",
        "            nn.Dropout(dropout), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"r2plus1d\":\n",
        "        model = torch.hub.load(\"pytorch/vision\", \"r2plus1d_18\", pretrained=True)\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"r3d_18\":\n",
        "        model = torch.hub.load(\"pytorch/vision\", \"r3d_18\", pretrained=True)\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"slow_r50\":\n",
        "        model = torch.hub.load(\n",
        "            \"facebookresearch/pytorchvideo\", \"slow_r50\", pretrained=True\n",
        "        )\n",
        "        in_features = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Sequential(\n",
        "            nn.Dropout(dropout), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"slowfast_r50\":\n",
        "        model = torch.hub.load(\n",
        "            \"facebookresearch/pytorchvideo\", \"slowfast_r50\", pretrained=True\n",
        "        )\n",
        "        in_features = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Sequential(\n",
        "            nn.Dropout(dropout), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    # CNN-RNN models\n",
        "    elif model_name == \"cnn_lstm\":\n",
        "        model = CNN_LSTM(\n",
        "            num_classes=num_classes, hidden_dim=512, num_layers=2, dropout=dropout\n",
        "        )\n",
        "\n",
        "    elif model_name == \"cnn_gru\":\n",
        "        model = CNN_GRU(\n",
        "            num_classes=num_classes, hidden_dim=512, num_layers=2, dropout=dropout\n",
        "        )\n",
        "\n",
        "    # Transformer models\n",
        "    elif model_name == \"timesformer\":\n",
        "        from timesformer_pytorch import TimeSformer\n",
        "\n",
        "        class TimeSformerWrapper(nn.Module):\n",
        "            \"\"\"Wrapper to permute input tensor dimensions for TimeSformer.\"\"\"\n",
        "\n",
        "            def __init__(self, model):\n",
        "                super().__init__()\n",
        "                self.model = model\n",
        "\n",
        "            def forward(self, x):\n",
        "                # Permute from (B, C, T, H, W) to (B, T, C, H, W)\n",
        "                return self.model(x.permute(0, 2, 1, 3, 4))\n",
        "\n",
        "        timesformer_model = TimeSformer(\n",
        "            dim=512,\n",
        "            image_size=224,\n",
        "            patch_size=16,\n",
        "            num_frames=clip_len,\n",
        "            num_classes=num_classes,\n",
        "            depth=12,\n",
        "            heads=8,\n",
        "            dim_head=64,\n",
        "            attn_dropout=dropout,\n",
        "            ff_dropout=dropout,\n",
        "        )\n",
        "        model = TimeSformerWrapper(timesformer_model)\n",
        "\n",
        "    elif model_name == \"mvit\":\n",
        "        model = MViT(\n",
        "            num_classes=num_classes,\n",
        "            img_size=224,\n",
        "            patch_size=16,\n",
        "            num_frames=clip_len,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            num_heads=12,\n",
        "            mlp_ratio=4.0,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    elif model_name == \"videomae\":\n",
        "        model = VideoMAE(\n",
        "            num_classes=num_classes,\n",
        "            img_size=224,\n",
        "            patch_size=16,\n",
        "            num_frames=clip_len,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            num_heads=12,\n",
        "            decoder_embed_dim=512,\n",
        "            decoder_depth=4,\n",
        "            decoder_num_heads=8,\n",
        "            mlp_ratio=4.0,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    elif model_name == \"vivit\":\n",
        "        model = ViViT(\n",
        "            num_classes=num_classes,\n",
        "            img_size=224,\n",
        "            patch_size=16,\n",
        "            num_frames=clip_len,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            num_heads=12,\n",
        "            mlp_ratio=4.0,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    # Freeze backbone if requested (except for transformer models and CNN-RNN)\n",
        "    if freeze_backbone and model_name not in [\n",
        "        \"timesformer\",\n",
        "        \"mvit\",\n",
        "        \"videomae\",\n",
        "        \"vivit\",\n",
        "        \"cnn_lstm\",\n",
        "        \"cnn_gru\",\n",
        "    ]:\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"proj\" not in name and \"fc\" not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 8️⃣ ───── DYNAMIC BATCH TUNER ─────\n",
        "def tune_batch_size(\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    initial_batch_size: int,\n",
        "    clip_len: int,\n",
        "    model_name: str,\n",
        ") -> Tuple[int, int]:\n",
        "    \"\"\"Dynamically tune batch size and clip length to fit in GPU memory.\"\"\"\n",
        "    model.eval()\n",
        "    batch_size = initial_batch_size\n",
        "    current_clip_len = clip_len\n",
        "\n",
        "    # Adjust initial batch size based on model type\n",
        "    if model_name in [\"timesformer\", \"mvit\", \"videomae\", \"vivit\"]:\n",
        "        batch_size = max(1, batch_size // 2)  # Transformers need more memory\n",
        "\n",
        "    # SlowFast and Slow models need at least 32 frames\n",
        "    if model_name in [\"slowfast_r50\", \"slow_r50\"]:\n",
        "        min_clip_len = 32\n",
        "    else:\n",
        "        min_clip_len = 8\n",
        "\n",
        "    while batch_size >= 1 and current_clip_len >= min_clip_len:\n",
        "        try:\n",
        "            # Test forward pass\n",
        "            dummy_input = torch.randn(batch_size, 3, current_clip_len, 224, 224).to(\n",
        "                device\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                if model_name.startswith(\"slowfast\"):\n",
        "                    alpha = 4\n",
        "                    slow_pathway = dummy_input[:, :, ::alpha, :, :]\n",
        "                    fast_pathway = dummy_input\n",
        "                    model_inputs = [slow_pathway, fast_pathway]\n",
        "                    _ = model(model_inputs)\n",
        "                else:\n",
        "                    _ = model(dummy_input)\n",
        "\n",
        "            console.print(\n",
        "                f\"✅ Optimal batch_size: {batch_size}, clip_len: {current_clip_len}\",\n",
        "                style=\"green\",\n",
        "            )\n",
        "            return batch_size, current_clip_len\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e) or \"smaller than kernel size\" in str(e):\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                if \"smaller than kernel size\" in str(e) and model_name in [\n",
        "                    \"slowfast_r50\",\n",
        "                    \"slow_r50\",\n",
        "                ]:\n",
        "                    # SlowFast/Slow models need more frames, double the clip length\n",
        "                    current_clip_len *= 2\n",
        "                    console.print(\n",
        "                        f\"⚠️ {model_name} needs more frames, increasing clip_len to {current_clip_len}\",\n",
        "                        style=\"yellow\",\n",
        "                    )\n",
        "                elif batch_size > 1:\n",
        "                    batch_size //= 2\n",
        "                    console.print(\n",
        "                        f\"⚠️ OOM detected, reducing batch_size to {batch_size}\",\n",
        "                        style=\"yellow\",\n",
        "                    )\n",
        "                else:\n",
        "                    current_clip_len //= 2\n",
        "                    batch_size = initial_batch_size\n",
        "                    console.print(\n",
        "                        f\"⚠️ OOM detected, reducing clip_len to {current_clip_len}\",\n",
        "                        style=\"yellow\",\n",
        "                    )\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    console.print(\n",
        "        \"❌ Could not find suitable batch_size/clip_len combination\", style=\"red\"\n",
        "    )\n",
        "    return 1, min_clip_len\n",
        "\n",
        "\n",
        "# 9️⃣ ───── TRAINING & VALIDATION LOOPS (WITH DETAILED LOGGING) ─────\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 5, min_delta: float = 0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "\n",
        "    def __call__(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle metadata.\"\"\"\n",
        "    videos = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.tensor([item[1] for item in batch])\n",
        "    metadata = [item[2] for item in batch]\n",
        "    return videos, labels, metadata\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    scaler: amp.GradScaler,\n",
        "    config: Dict,\n",
        "    epoch: int,\n",
        "    output_dirs: Dict,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Train for one epoch with detailed logging.\"\"\"\n",
        "    model.train()\n",
        "    metrics = {name: [] for name in config[\"metrics\"]}\n",
        "\n",
        "    # For detailed logging\n",
        "    epoch_predictions = []\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"Training...\", total=len(dataloader))\n",
        "\n",
        "        for batch_idx, (videos, labels, metadata) in enumerate(dataloader):\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with amp.autocast(enabled=config[\"hardware\"][\"mixed_precision\"]):\n",
        "                if config[\"model\"][\"model_name\"].startswith(\"slowfast\"):\n",
        "                    alpha = 4\n",
        "                    slow_pathway = videos[:, :, ::alpha, :, :]\n",
        "                    fast_pathway = videos\n",
        "                    model_inputs = [slow_pathway, fast_pathway]\n",
        "                    outputs = model(model_inputs)\n",
        "                else:\n",
        "                    outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Calculate metrics and save predictions\n",
        "            with torch.no_grad():\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "                # Save detailed predictions\n",
        "                for i, meta in enumerate(metadata):\n",
        "                    epoch_predictions.append(\n",
        "                        {\n",
        "                            \"batch_idx\": batch_idx,\n",
        "                            \"video_path\": meta[\"video_path\"],\n",
        "                            \"snippet_idx\": meta[\"snippet_idx\"],\n",
        "                            \"class_name\": meta[\"class_name\"],\n",
        "                            \"true_label\": labels[i].item(),\n",
        "                            \"predicted_label\": preds[i].item(),\n",
        "                            \"probabilities\": probs[i].cpu().numpy().tolist(),\n",
        "                            \"loss\": loss.item(),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                metrics[\"loss\"].append(loss.item())\n",
        "                metrics[\"accuracy\"].append((preds == labels).float().mean().item())\n",
        "\n",
        "                # Binary classification metrics\n",
        "                if len(torch.unique(labels)) <= 2:\n",
        "                    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
        "                    tn = ((preds == 0) & (labels == 0)).sum().item()\n",
        "                    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
        "                    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                    f1 = (\n",
        "                        2 * precision * recall / (precision + recall)\n",
        "                        if (precision + recall) > 0\n",
        "                        else 0\n",
        "                    )\n",
        "                    sensitivity = recall\n",
        "                    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "                    metrics[\"precision\"].append(precision)\n",
        "                    metrics[\"recall\"].append(recall)\n",
        "                    metrics[\"f1\"].append(f1)\n",
        "                    metrics[\"sensitivity\"].append(sensitivity)\n",
        "                    metrics[\"specificity\"].append(specificity)\n",
        "                    metrics[\"auc\"].append(0.5)  # Placeholder\n",
        "\n",
        "            progress.advance(task)\n",
        "\n",
        "            if batch_idx % config[\"logging\"][\"print_freq\"] == 0:\n",
        "                emoji = \"🔥\" if config[\"logging\"][\"emojis\"] else \"\"\n",
        "                console.print(\n",
        "                    f\"{emoji} Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "    # Save detailed predictions\n",
        "    if config[\"logging\"][\"save_detailed_predictions\"]:\n",
        "        predictions_file = (\n",
        "            output_dirs[\"predictions\"] / f\"train_epoch_{epoch}_predictions.json\"\n",
        "        )\n",
        "        with open(predictions_file, \"w\") as f:\n",
        "            json.dump(epoch_predictions, f, indent=2)\n",
        "\n",
        "    return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    config: Dict,\n",
        "    epoch: int,\n",
        "    output_dirs: Dict,\n",
        "    split_name: str = \"val\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Validate for one epoch with detailed logging.\"\"\"\n",
        "    model.eval()\n",
        "    metrics = {name: [] for name in config[\"metrics\"]}\n",
        "\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "    epoch_predictions = []\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\n",
        "            f\"{split_name.capitalize()}ating...\", total=len(dataloader)\n",
        "        )\n",
        "\n",
        "        for batch_idx, (videos, labels, metadata) in enumerate(dataloader):\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "            with amp.autocast(enabled=config[\"hardware\"][\"mixed_precision\"]):\n",
        "                if config[\"model\"][\"model_name\"].startswith(\"slowfast\"):\n",
        "                    alpha = 4\n",
        "                    slow_pathway = videos[:, :, ::alpha, :, :]\n",
        "                    fast_pathway = videos\n",
        "                    model_inputs = [slow_pathway, fast_pathway]\n",
        "                    outputs = model(model_inputs)\n",
        "                else:\n",
        "                    outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            for i, meta in enumerate(metadata):\n",
        "                epoch_predictions.append(\n",
        "                    {\n",
        "                        \"batch_idx\": batch_idx,\n",
        "                        \"video_path\": meta[\"video_path\"],\n",
        "                        \"snippet_idx\": meta[\"snippet_idx\"],\n",
        "                        \"class_name\": meta[\"class_name\"],\n",
        "                        \"true_label\": labels[i].item(),\n",
        "                        \"predicted_label\": preds[i].item(),\n",
        "                        \"probabilities\": probs[i].cpu().numpy().tolist(),\n",
        "                        \"loss\": loss.item(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            metrics[\"loss\"].append(loss.item())\n",
        "            metrics[\"accuracy\"].append((preds == labels).float().mean().item())\n",
        "\n",
        "            progress.advance(task)\n",
        "\n",
        "    # Save detailed predictions\n",
        "    if config[\"logging\"][\"save_detailed_predictions\"]:\n",
        "        predictions_file = (\n",
        "            output_dirs[\"predictions\"] / f\"{split_name}_epoch_{epoch}_predictions.json\"\n",
        "        )\n",
        "        with open(predictions_file, \"w\") as f:\n",
        "            json.dump(epoch_predictions, f, indent=2)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # Binary classification metrics\n",
        "    if len(np.unique(all_labels)) <= 2:\n",
        "        tp = ((all_preds == 1) & (all_labels == 1)).sum()\n",
        "        tn = ((all_preds == 0) & (all_labels == 0)).sum()\n",
        "        fp = ((all_preds == 1) & (all_labels == 0)).sum()\n",
        "        fn = ((all_preds == 0) & (all_labels == 1)).sum()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision + recall) > 0\n",
        "            else 0\n",
        "        )\n",
        "        sensitivity = recall\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "\n",
        "        auc = (\n",
        "            roc_auc_score(all_labels, all_probs[:, 1])\n",
        "            if all_probs.shape[1] == 2\n",
        "            else 0.5\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"loss\": np.mean(metrics[\"loss\"]),\n",
        "            \"accuracy\": np.mean(metrics[\"accuracy\"]),\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"sensitivity\": sensitivity,\n",
        "            \"specificity\": specificity,\n",
        "            \"auc\": auc,\n",
        "        }\n",
        "\n",
        "    return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "# 🔟 ───── PLOT PDFS ─────\n",
        "def create_plots(\n",
        "    train_history: Dict, val_history: Dict, output_dirs: Dict, config: Dict\n",
        ") -> None:\n",
        "    \"\"\"Create training plots and save as PDFs.\"\"\"\n",
        "    plt.style.use(\"default\")\n",
        "\n",
        "    # Create a single figure with subplots for all metrics\n",
        "    n_metrics = len(config[\"metrics\"])\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, metric in enumerate(config[\"metrics\"]):\n",
        "        if metric in train_history and metric in val_history:\n",
        "            ax = axes[idx]\n",
        "            epochs = range(1, len(train_history[metric]) + 1)\n",
        "\n",
        "            ax.plot(epochs, train_history[metric], \"b-\", label=f\"Train\", linewidth=2)\n",
        "            ax.plot(epochs, val_history[metric], \"r-\", label=f\"Val\", linewidth=2)\n",
        "\n",
        "            ax.set_title(f\"{metric.title()}\", fontsize=14, fontweight=\"bold\")\n",
        "            ax.set_xlabel(\"Epochs\", fontsize=12)\n",
        "            ax.set_ylabel(metric.title(), fontsize=12)\n",
        "            ax.legend(fontsize=10)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_metrics, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        output_dirs[\"plots\"] / \"all_metrics.pdf\",\n",
        "        format=\"pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "    # Also save individual metric plots\n",
        "    for metric in config[\"metrics\"]:\n",
        "        if metric in train_history and metric in val_history:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            epochs = range(1, len(train_history[metric]) + 1)\n",
        "\n",
        "            plt.plot(\n",
        "                epochs,\n",
        "                train_history[metric],\n",
        "                \"b-\",\n",
        "                label=f\"Train {metric.title()}\",\n",
        "                linewidth=2,\n",
        "            )\n",
        "            plt.plot(\n",
        "                epochs,\n",
        "                val_history[metric],\n",
        "                \"r-\",\n",
        "                label=f\"Val {metric.title()}\",\n",
        "                linewidth=2,\n",
        "            )\n",
        "\n",
        "            plt.title(f\"{metric.title()} vs Epochs\", fontsize=16, fontweight=\"bold\")\n",
        "            plt.xlabel(\"Epochs\", fontsize=12)\n",
        "            plt.ylabel(metric.title(), fontsize=12)\n",
        "            plt.legend(fontsize=12)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.savefig(\n",
        "                output_dirs[\"plots\"] / f\"{metric}.pdf\",\n",
        "                format=\"pdf\",\n",
        "                bbox_inches=\"tight\",\n",
        "                dpi=300,\n",
        "            )\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "def create_confusion_matrix(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str], output_dirs: Dict\n",
        ") -> None:\n",
        "    \"\"\"Create confusion matrix plots.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Counts confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix (Counts)\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.ylabel(\"True Label\", fontsize=12)\n",
        "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "    plt.savefig(\n",
        "        output_dirs[\"plots\"] / \"confmat_counts.pdf\",\n",
        "        format=\"pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "    # Percentage confusion matrix\n",
        "    cm_pct = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm_pct,\n",
        "        annot=True,\n",
        "        fmt=\".1f\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix (Percentages)\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.ylabel(\"True Label\", fontsize=12)\n",
        "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "    plt.savefig(\n",
        "        output_dirs[\"plots\"] / \"confmat_pct.pdf\",\n",
        "        format=\"pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ❶❶ ───── INFERENCE HELPER ─────\n",
        "@torch.no_grad()\n",
        "def run_inference(\n",
        "    model: nn.Module,\n",
        "    video_path: str,\n",
        "    device: torch.device,\n",
        "    class_names: List[str],\n",
        "    config: Dict,\n",
        ") -> Dict:\n",
        "    \"\"\"Run inference on a single video.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    dataset = VideoDataset(\n",
        "        [{\"video_path\": video_path, \"class_idx\": 0, \"class_name\": \"unknown\"}],\n",
        "        config[\"data\"][\"clip_len\"],\n",
        "        config[\"data\"][\"frame_rate\"],\n",
        "        overlap=0,  # No overlap for single inference\n",
        "        augment=False,\n",
        "    )\n",
        "\n",
        "    video_tensor, _, metadata = dataset[0]\n",
        "    video_tensor = video_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with amp.autocast(enabled=config[\"hardware\"][\"mixed_precision\"]):\n",
        "        if config[\"model\"][\"model_name\"].startswith(\"slowfast\"):\n",
        "            alpha = 4\n",
        "            slow_pathway = video_tensor[:, :, ::alpha, :, :]\n",
        "            fast_pathway = video_tensor\n",
        "            model_inputs = [slow_pathway, fast_pathway]\n",
        "            outputs = model(model_inputs)\n",
        "        else:\n",
        "            outputs = model(video_tensor)\n",
        "\n",
        "    probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "    pred_class = np.argmax(probs)\n",
        "\n",
        "    results = {\n",
        "        \"predicted_class\": class_names[pred_class],\n",
        "        \"confidence\": float(probs[pred_class]),\n",
        "        \"all_probabilities\": {\n",
        "            class_names[i]: float(probs[i]) for i in range(len(class_names))\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ❶❷ ───── MAIN FUNCTION ─────\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    print_section(\"INITIALIZATION\", \"🚀\")\n",
        "\n",
        "    # Seed everything\n",
        "    seed_everything(config[\"train\"][\"seed\"])\n",
        "\n",
        "    # Check GPU\n",
        "    has_gpu, gpu_mem = check_gpu_memory()\n",
        "    if not has_gpu and config[\"hardware\"][\"gpus\"] > 0:\n",
        "        console.print(\n",
        "            \"⚠️ No GPU detected but GPU requested. Switching to CPU.\", style=\"yellow\"\n",
        "        )\n",
        "        device = torch.device(\"cpu\")\n",
        "    else:\n",
        "        device = torch.device(\n",
        "            \"cuda\" if has_gpu and config[\"hardware\"][\"gpus\"] > 0 else \"cpu\"\n",
        "        )\n",
        "\n",
        "    console.print(f\"🖥️ Device: {device}\")\n",
        "    if has_gpu:\n",
        "        console.print(f\"💾 GPU Memory: {gpu_mem:.1f} GB\")\n",
        "\n",
        "    # Setup output directories\n",
        "    output_dirs = setup_output_dirs(config[\"paths\"][\"output_root\"])\n",
        "    console.print(f\"📁 Output directory: {output_dirs['root']}\")\n",
        "\n",
        "    # Save config\n",
        "    with open(output_dirs[\"root\"] / \"config.yaml\", \"w\") as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "    print_section(\"DATA PREPARATION\", \"📊\")\n",
        "\n",
        "    # Create splits\n",
        "    splits_info = create_splits(config[\"paths\"][\"data_root\"], config)\n",
        "\n",
        "    # Save splits\n",
        "    with open(output_dirs[\"root\"] / \"splits.json\", \"w\") as f:\n",
        "        json.dump(splits_info, f, indent=2)\n",
        "\n",
        "    class_names = splits_info[\"class_names\"]\n",
        "    num_classes = splits_info[\"num_classes\"]\n",
        "\n",
        "    console.print(f\"📋 Classes: {class_names}\")\n",
        "    console.print(f\"🔢 Number of classes: {num_classes}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = VideoDataset(\n",
        "        splits_info[\"splits\"][\"train\"],\n",
        "        config[\"data\"][\"clip_len\"],\n",
        "        config[\"data\"][\"frame_rate\"],\n",
        "        config[\"data\"][\"snippet_overlap\"],\n",
        "        augment=False,\n",
        "    )\n",
        "    val_dataset = VideoDataset(\n",
        "        splits_info[\"splits\"][\"val\"],\n",
        "        config[\"data\"][\"clip_len\"],\n",
        "        config[\"data\"][\"frame_rate\"],\n",
        "        config[\"data\"][\"snippet_overlap\"],\n",
        "        augment=False,\n",
        "    )\n",
        "    test_dataset = VideoDataset(\n",
        "        splits_info[\"splits\"][\"test\"],\n",
        "        config[\"data\"][\"clip_len\"],\n",
        "        config[\"data\"][\"frame_rate\"],\n",
        "        config[\"data\"][\"snippet_overlap\"],\n",
        "        augment=False,\n",
        "    )\n",
        "\n",
        "    console.print(f\"📈 Train snippets: {len(train_dataset)}\")\n",
        "    console.print(f\"📊 Val snippets: {len(val_dataset)}\")\n",
        "    console.print(f\"📉 Test snippets: {len(test_dataset)}\")\n",
        "\n",
        "    print_section(\"MODEL CREATION\", \"🧠\")\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(\n",
        "        config[\"model\"][\"model_name\"],\n",
        "        num_classes,\n",
        "        config[\"data\"][\"clip_len\"],\n",
        "        config[\"model\"][\"freeze_backbone\"],\n",
        "        config[\"model\"][\"dropout\"],\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    console.print(f\"🎯 Model: {config['model']['model_name']}\")\n",
        "    console.print(f\"🔒 Freeze backbone: {config['model']['freeze_backbone']}\")\n",
        "\n",
        "    # Dynamic batch size tuning\n",
        "    if device.type == \"cuda\":\n",
        "        original_clip_len = config[\"data\"][\"clip_len\"]\n",
        "        optimal_batch_size, optimal_clip_len = tune_batch_size(\n",
        "            model,\n",
        "            device,\n",
        "            config[\"train\"][\"batch_size\"],\n",
        "            original_clip_len,\n",
        "            config[\"model\"][\"model_name\"],\n",
        "        )\n",
        "\n",
        "        config[\"train\"][\"batch_size\"] = optimal_batch_size\n",
        "\n",
        "        if optimal_clip_len != original_clip_len:\n",
        "            console.print(\n",
        "                f\"🔄 Clip length auto-adjusted from {original_clip_len} to {optimal_clip_len}\",\n",
        "                style=\"yellow\",\n",
        "            )\n",
        "            config[\"data\"][\"clip_len\"] = optimal_clip_len\n",
        "\n",
        "            # Recreate datasets with new clip length\n",
        "            train_dataset = VideoDataset(\n",
        "                splits_info[\"splits\"][\"train\"],\n",
        "                optimal_clip_len,\n",
        "                config[\"data\"][\"frame_rate\"],\n",
        "                config[\"data\"][\"snippet_overlap\"],\n",
        "                augment=False,\n",
        "            )\n",
        "            val_dataset = VideoDataset(\n",
        "                splits_info[\"splits\"][\"val\"],\n",
        "                optimal_clip_len,\n",
        "                config[\"data\"][\"frame_rate\"],\n",
        "                config[\"data\"][\"snippet_overlap\"],\n",
        "                augment=False,\n",
        "            )\n",
        "            test_dataset = VideoDataset(\n",
        "                splits_info[\"splits\"][\"test\"],\n",
        "                optimal_clip_len,\n",
        "                config[\"data\"][\"frame_rate\"],\n",
        "                config[\"data\"][\"snippet_overlap\"],\n",
        "                augment=False,\n",
        "            )\n",
        "\n",
        "            # Recreate model if needed (for transformers that depend on clip_len)\n",
        "            if config[\"model\"][\"model_name\"] in [\n",
        "                \"timesformer\",\n",
        "                \"mvit\",\n",
        "                \"videomae\",\n",
        "                \"vivit\",\n",
        "            ]:\n",
        "                model = create_model(\n",
        "                    config[\"model\"][\"model_name\"],\n",
        "                    num_classes,\n",
        "                    optimal_clip_len,\n",
        "                    config[\"model\"][\"freeze_backbone\"],\n",
        "                    config[\"model\"][\"dropout\"],\n",
        "                )\n",
        "                model = model.to(device)\n",
        "\n",
        "    # Create data loaders with custom collate function\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"train\"][\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=config[\"data\"][\"num_workers\"],\n",
        "        pin_memory=True if device.type == \"cuda\" else False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"train\"][\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=config[\"data\"][\"num_workers\"],\n",
        "        pin_memory=True if device.type == \"cuda\" else False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"train\"][\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=config[\"data\"][\"num_workers\"],\n",
        "        pin_memory=True if device.type == \"cuda\" else False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    if config[\"modes\"][\"run_training\"]:\n",
        "        print_section(\"TRAINING\", \"🏋️\")\n",
        "\n",
        "        # Setup training components\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config[\"train\"][\"lr\"],\n",
        "            weight_decay=config[\"train\"][\"weight_decay\"],\n",
        "        )\n",
        "\n",
        "        if config[\"train\"][\"scheduler\"] == \"cosine\":\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "                optimizer, T_max=config[\"train\"][\"epochs\"]\n",
        "            )\n",
        "        else:\n",
        "            scheduler = optim.lr_scheduler.StepLR(\n",
        "                optimizer,\n",
        "                step_size=config[\"train\"][\"step_size\"],\n",
        "                gamma=config[\"train\"][\"gamma\"],\n",
        "            )\n",
        "\n",
        "        scaler = amp.GradScaler(enabled=config[\"hardware\"][\"mixed_precision\"])\n",
        "        early_stopping = EarlyStopping(patience=config[\"train\"][\"early_stop_patience\"])\n",
        "\n",
        "        # Training history\n",
        "        train_history = {metric: [] for metric in config[\"metrics\"]}\n",
        "        val_history = {metric: [] for metric in config[\"metrics\"]}\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(config[\"train\"][\"epochs\"]):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            emoji = \"🔥\" if config[\"logging\"][\"emojis\"] else \"\"\n",
        "            console.print(\n",
        "                f\"\\n{emoji} Epoch {epoch+1}/{config['train']['epochs']}\",\n",
        "                style=\"bold magenta\",\n",
        "            )\n",
        "\n",
        "            # Train\n",
        "            train_metrics = train_epoch(\n",
        "                model,\n",
        "                train_loader,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                device,\n",
        "                scaler,\n",
        "                config,\n",
        "                epoch + 1,\n",
        "                output_dirs,\n",
        "            )\n",
        "\n",
        "            # Validate\n",
        "            val_metrics = validate_epoch(\n",
        "                model,\n",
        "                val_loader,\n",
        "                criterion,\n",
        "                device,\n",
        "                config,\n",
        "                epoch + 1,\n",
        "                output_dirs,\n",
        "                \"val\",\n",
        "            )\n",
        "\n",
        "            # Update history\n",
        "            for metric in config[\"metrics\"]:\n",
        "                if metric in train_metrics:\n",
        "                    train_history[metric].append(train_metrics[metric])\n",
        "                if metric in val_metrics:\n",
        "                    val_history[metric].append(val_metrics[metric])\n",
        "\n",
        "            # Learning rate step\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print epoch summary\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "            table = Table(title=f\"Epoch {epoch+1} Summary\")\n",
        "            table.add_column(\"Metric\", style=\"cyan\")\n",
        "            table.add_column(\"Train\", style=\"green\")\n",
        "            table.add_column(\"Val\", style=\"red\")\n",
        "\n",
        "            for metric in config[\"metrics\"]:\n",
        "                if metric in train_metrics and metric in val_metrics:\n",
        "                    table.add_row(\n",
        "                        metric.title(),\n",
        "                        f\"{train_metrics[metric]:.4f}\",\n",
        "                        f\"{val_metrics[metric]:.4f}\",\n",
        "                    )\n",
        "\n",
        "            console.print(table)\n",
        "            console.print(f\"⏱️ Epoch time: {epoch_time:.2f}s\")\n",
        "\n",
        "            # Save checkpoint every epoch\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                    \"train_metrics\": train_metrics,\n",
        "                    \"val_metrics\": val_metrics,\n",
        "                    \"config\": config,\n",
        "                },\n",
        "                output_dirs[\"checkpoints\"] / f\"epoch_{epoch+1}.pth\",\n",
        "            )\n",
        "\n",
        "            # Save best model\n",
        "            if val_metrics[\"loss\"] < best_val_loss:\n",
        "                best_val_loss = val_metrics[\"loss\"]\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"epoch\": epoch + 1,\n",
        "                        \"model_state_dict\": model.state_dict(),\n",
        "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                        \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                        \"val_loss\": val_metrics[\"loss\"],\n",
        "                        \"config\": config,\n",
        "                    },\n",
        "                    output_dirs[\"checkpoints\"] / \"best.pth\",\n",
        "                )\n",
        "\n",
        "                star = \"⭐\" if config[\"logging\"][\"emojis\"] else \"*\"\n",
        "                console.print(f\"{star} New best model saved!\", style=\"bold green\")\n",
        "\n",
        "            # Save training history\n",
        "            history_data = {\n",
        "                \"train\": train_history,\n",
        "                \"val\": val_history,\n",
        "                \"current_epoch\": epoch + 1,\n",
        "            }\n",
        "            with open(output_dirs[\"logs\"] / \"training_history.json\", \"w\") as f:\n",
        "                json.dump(history_data, f, indent=2)\n",
        "\n",
        "            # Early stopping\n",
        "            if early_stopping(val_metrics[\"loss\"]):\n",
        "                stop_emoji = \"🛑\" if config[\"logging\"][\"emojis\"] else \"STOP\"\n",
        "                console.print(\n",
        "                    f\"{stop_emoji} Early stopping triggered!\", style=\"bold red\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "        # Create training plots\n",
        "        create_plots(train_history, val_history, output_dirs, config)\n",
        "\n",
        "        console.print(\"✅ Training completed!\", style=\"bold green\")\n",
        "\n",
        "    if config[\"modes\"][\"run_eval\"]:\n",
        "        print_section(\"EVALUATION\", \"📊\")\n",
        "\n",
        "        # Load best model\n",
        "        checkpoint = torch.load(\n",
        "            output_dirs[\"checkpoints\"] / \"best.pth\",\n",
        "            map_location=device,\n",
        "            weights_only=False,\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "        console.print(\"✅ Best model loaded for evaluation\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        test_metrics = validate_epoch(\n",
        "            model,\n",
        "            test_loader,\n",
        "            criterion,\n",
        "            device,\n",
        "            config,\n",
        "            checkpoint[\"epoch\"],\n",
        "            output_dirs,\n",
        "            \"test\",\n",
        "        )\n",
        "\n",
        "        # Get predictions for confusion matrix\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for videos, labels, metadata in test_loader:\n",
        "                videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "                with amp.autocast(enabled=config[\"hardware\"][\"mixed_precision\"]):\n",
        "                    if config[\"model\"][\"model_name\"].startswith(\"slowfast\"):\n",
        "                        alpha = 4\n",
        "                        slow_pathway = videos[:, :, ::alpha, :, :]\n",
        "                        fast_pathway = videos\n",
        "                        model_inputs = [slow_pathway, fast_pathway]\n",
        "                        outputs = model(model_inputs)\n",
        "                    else:\n",
        "                        outputs = model(videos)\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Create confusion matrix\n",
        "        create_confusion_matrix(\n",
        "            np.array(all_labels), np.array(all_preds), class_names, output_dirs\n",
        "        )\n",
        "\n",
        "        # Save test metrics\n",
        "        metrics_df = pd.DataFrame([test_metrics])\n",
        "        metrics_df.to_csv(output_dirs[\"root\"] / \"test_metrics.csv\", index=False)\n",
        "\n",
        "        # Print test results\n",
        "        table = Table(title=\"Test Set Results\")\n",
        "        table.add_column(\"Metric\", style=\"cyan\")\n",
        "        table.add_column(\"Value\", style=\"green\")\n",
        "\n",
        "        for metric, value in test_metrics.items():\n",
        "            table.add_row(metric.title(), f\"{value:.4f}\")\n",
        "\n",
        "        console.print(table)\n",
        "\n",
        "        # Classification report\n",
        "        from sklearn.metrics import classification_report\n",
        "\n",
        "        report = classification_report(\n",
        "            all_labels, all_preds, target_names=class_names, output_dict=True\n",
        "        )\n",
        "\n",
        "        # Save classification report\n",
        "        with open(output_dirs[\"logs\"] / \"classification_report.json\", \"w\") as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        console.print(\"\\n📋 Detailed Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "        console.print(\"✅ Evaluation completed!\", style=\"bold green\")\n",
        "\n",
        "    if config[\"modes\"][\"run_inference\"]:\n",
        "        print_section(\"INFERENCE DEMO\", \"🎯\")\n",
        "\n",
        "        inference_video = config[\"modes\"][\"inference_video\"]\n",
        "\n",
        "        if os.path.exists(inference_video):\n",
        "            # Load best model if not already loaded\n",
        "            if not config[\"modes\"][\"run_eval\"]:\n",
        "                checkpoint = torch.load(\n",
        "                    output_dirs[\"checkpoints\"] / \"best.pth\",\n",
        "                    map_location=device,\n",
        "                    weights_only=False,\n",
        "                )\n",
        "                model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "            # Run inference\n",
        "            results = run_inference(model, inference_video, device, class_names, config)\n",
        "\n",
        "            # Display results\n",
        "            table = Table(title=f\"Inference Results\")\n",
        "            table.add_column(\"Class\", style=\"cyan\")\n",
        "            table.add_column(\"Probability\", style=\"green\")\n",
        "\n",
        "            for class_name, prob in results[\"all_probabilities\"].items():\n",
        "                style = (\n",
        "                    \"bold green\"\n",
        "                    if class_name == results[\"predicted_class\"]\n",
        "                    else \"white\"\n",
        "                )\n",
        "                table.add_row(class_name, f\"{prob:.4f}\", style=style)\n",
        "\n",
        "            console.print(table)\n",
        "\n",
        "            pred_emoji = \"🎯\" if config[\"logging\"][\"emojis\"] else \">>>\"\n",
        "            console.print(\n",
        "                f\"{pred_emoji} Predicted: {results['predicted_class']} \"\n",
        "                f\"(Confidence: {results['confidence']:.4f})\",\n",
        "                style=\"bold green\",\n",
        "            )\n",
        "\n",
        "            # Save inference results\n",
        "            inference_results = {\n",
        "                \"video_path\": inference_video,\n",
        "                \"results\": results,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "            }\n",
        "            with open(output_dirs[\"logs\"] / \"inference_results.json\", \"w\") as f:\n",
        "                json.dump(inference_results, f, indent=2)\n",
        "        else:\n",
        "            console.print(\n",
        "                f\"❌ Inference video not found: {inference_video}\", style=\"red\"\n",
        "            )\n",
        "\n",
        "    print_section(\"COMPLETION\", \"🎉\")\n",
        "\n",
        "    # Summary\n",
        "    console.print(\"🏁 All tasks completed successfully!\", style=\"bold green\")\n",
        "    console.print(f\"📁 Check outputs at: {output_dirs['root']}\")\n",
        "\n",
        "    # Final summary report\n",
        "    summary = {\n",
        "        \"run_timestamp\": datetime.now().isoformat(),\n",
        "        \"output_directory\": str(output_dirs[\"root\"]),\n",
        "        \"config\": config,\n",
        "        \"splits_info\": {\n",
        "            \"num_classes\": splits_info[\"num_classes\"],\n",
        "            \"class_names\": splits_info[\"class_names\"],\n",
        "            \"videos_per_class\": splits_info[\"videos_per_class\"],\n",
        "            \"split_mode\": splits_info[\"split_mode\"],\n",
        "        },\n",
        "        \"model_info\": {\n",
        "            \"model_name\": config[\"model\"][\"model_name\"],\n",
        "            \"num_classes\": num_classes,\n",
        "            \"clip_len\": config[\"data\"][\"clip_len\"],\n",
        "            \"batch_size\": config[\"train\"][\"batch_size\"],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    with open(output_dirs[\"root\"] / \"run_summary.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    # Print file tree\n",
        "    tree_emoji = \"🌳\" if config[\"logging\"][\"emojis\"] else \"TREE\"\n",
        "    console.print(f\"\\n{tree_emoji} Output file structure:\")\n",
        "\n",
        "    def print_tree(path, prefix=\"\", is_last=True):\n",
        "        \"\"\"Print directory tree.\"\"\"\n",
        "        if path.is_file():\n",
        "            connector = \"└── \" if is_last else \"├── \"\n",
        "            console.print(f\"{prefix}{connector}{path.name}\")\n",
        "        elif path.is_dir():\n",
        "            connector = \"└── \" if is_last else \"├── \"\n",
        "            console.print(f\"{prefix}{connector}{path.name}/\")\n",
        "\n",
        "            children = list(path.iterdir())\n",
        "            for i, child in enumerate(children):\n",
        "                is_last_child = i == len(children) - 1\n",
        "                extension = \"    \" if is_last else \"│   \"\n",
        "                print_tree(child, prefix + extension, is_last_child)\n",
        "\n",
        "    print_tree(output_dirs[\"root\"])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}